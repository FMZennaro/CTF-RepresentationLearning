{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLi Tokenizer\n",
    "\n",
    "In this notebook we train a tokenizer and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect HTML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Studies\\PhD\\papers\\Capture_The_Flag_ML\\CTF-RepresentationLearning\\html\\correctescape.html\n",
      "D:\\Studies\\PhD\\papers\\Capture_The_Flag_ML\\CTF-RepresentationLearning\\html\\flag.html\n",
      "D:\\Studies\\PhD\\papers\\Capture_The_Flag_ML\\CTF-RepresentationLearning\\html\\jabbahome.html\n",
      "D:\\Studies\\PhD\\papers\\Capture_The_Flag_ML\\CTF-RepresentationLearning\\html\\wronglogin.html\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "for f in os.listdir(os.path.join(os.getcwd(),'html')):\n",
    "    print(os.path.join(os.getcwd(),'html',f))\n",
    "    fd = open(os.path.join(os.getcwd(),'html',f),'r')\n",
    "    content = fd.read()\n",
    "    files.append(content)\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!-- saved from url=(0042)http://jabba.hackingarena.no:806/index.php -->\\n<html><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"></head><body>Login as admin for the flag!\\n<br>some content, some content\\n<form action=\"http://jabba.hackingarena.no:806/index.php\" method=\"post\">\\n<table width=\"100\">\\n<tbody><tr><td>Name:</td>\\n<td><input type=\"text\" name=\"username\" value=\"\"></td></tr>\\n<tr><td>Password:</td>\\n<td><input type=\"text\" name=\"passwd\" value=\"\"></td></tr>\\n\\n<tr><td><input type=\"submit\" value=\"Submit\"></td></tr>\\n</tbody></table>\\n</form>\\n\\n</body></html>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a mock file as a concatentation of a previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.append(files[0]*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the tokenizer\n",
    "\n",
    "Define the tokenization, padding and trimming parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = False\n",
    "filters = '\"#$%&()*+,.;?@[\\\\]^_`{|}~\\t\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<!--',\n",
       " 'saved',\n",
       " 'from',\n",
       " 'url=',\n",
       " '0042',\n",
       " 'http://jabba',\n",
       " 'hackingarena',\n",
       " 'no:806/index',\n",
       " 'php',\n",
       " '-->',\n",
       " '<html><head><meta',\n",
       " 'http-equiv=',\n",
       " 'Content-Type',\n",
       " 'content=',\n",
       " 'text/html',\n",
       " 'charset=UTF-8',\n",
       " '></head><body>Login',\n",
       " 'as',\n",
       " 'admin',\n",
       " 'for',\n",
       " 'the',\n",
       " 'flag!',\n",
       " '<br>some',\n",
       " 'content',\n",
       " 'some',\n",
       " 'content',\n",
       " '<form',\n",
       " 'action=',\n",
       " 'http://jabba',\n",
       " 'hackingarena',\n",
       " 'no:806/index',\n",
       " 'php',\n",
       " 'method=',\n",
       " 'post',\n",
       " '>',\n",
       " '<table',\n",
       " 'width=',\n",
       " '100',\n",
       " '>',\n",
       " '<tbody><tr><td>Name:</td>',\n",
       " '<td><input',\n",
       " 'type=',\n",
       " 'text',\n",
       " 'name=',\n",
       " 'username',\n",
       " 'value=',\n",
       " '></td></tr>',\n",
       " '<tr><td>Password:</td>',\n",
       " '<td><input',\n",
       " 'type=',\n",
       " 'text',\n",
       " 'name=',\n",
       " 'passwd',\n",
       " 'value=',\n",
       " '></td></tr>',\n",
       " '<tr><td><input',\n",
       " 'type=',\n",
       " 'submit',\n",
       " 'value=',\n",
       " 'Submit',\n",
       " '></td></tr>',\n",
       " '</tbody></table>',\n",
       " '</form>',\n",
       " '</body></html>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(files[0],lower=lower, filters=filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the tokenizer\n",
    "\n",
    "Learn a tokenization dictionary and convert the pages into token sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLi_tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "SQLi_tokenizer.fit_on_texts(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the tokenizer\n",
    "\n",
    "Saving the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ignore_tokenizer_20210318153153509112']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "joblib.dump(SQLi_tokenizer,'ignore_tokenizer_'+timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
